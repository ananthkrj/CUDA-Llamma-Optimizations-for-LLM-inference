# CUDA-Llamma-Optimizations-for-LLM-inference
high performance RMSNorm + KV Cache, and Flash attention Kernels for LLM interface
