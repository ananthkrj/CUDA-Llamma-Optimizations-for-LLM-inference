# load lit llama checkpoint

# run inference on a batch of tokens on native pytorch
# rmsnorm vs my own kernel

# compare output similarity (torch.allclose)
# latency (nms), cuda memory usage, and throughput (token/sec)